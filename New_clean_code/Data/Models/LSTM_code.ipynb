{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "pmshiijttrVw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klxSwkvqtxcU",
        "outputId": "cd8392bd-7ed5-4ced-a598-8abb9b56bea5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding\n",
        "from keras.initializers import Constant"
      ],
      "metadata": {
        "id": "rQum3jsn0Bwo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWj7ydaZrGp1",
        "outputId": "efe067de-42b3-418a-cf4c-2e134cd72dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/703_Project/LSTM\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/703_Project/LSTM/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create combined csv files**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n8lPKfzW72qN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_folder(main_folder_path, group_label):\n",
        "    \"\"\" Traite tous les fichiers CSV dans un dossier et retourne un DataFrame fusionné. \"\"\"\n",
        "    frames = []  # Liste pour stocker les DataFrames de chaque fichier\n",
        "    for subdir, dirs, files in os.walk(main_folder_path):\n",
        "      for filename in files:\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(subdir, filename)\n",
        "            df = pd.read_csv(file_path)\n",
        "            df['conversation_id'] = filename.split('.')[0]  # Utilisez le nom du fichier comme ID\n",
        "            df['group'] = group_label\n",
        "            df = df[['text', 'conversation_id', 'group']]\n",
        "            frames.append(df)\n",
        "    return pd.concat(frames)\n",
        "\n",
        "# Chemins des dossiers pour les fichiers CSV des groupes contrôle et patient\n",
        "control_main_folder_path = \"Control_csv_data_clean\"\n",
        "patient_main_folder_path = \"Patient_csv_data_clean\"\n",
        "\n",
        "# Traitement des dossiers et fusion des DataFrames\n",
        "control_df = process_folder(control_main_folder_path, 'control')\n",
        "patient_df = process_folder(patient_main_folder_path, 'patient')\n",
        "combined_df = pd.concat([control_df, patient_df])\n",
        "\n",
        "# Enregistrement du DataFrame fusionné dans un nouveau fichier CSV\n",
        "combined_df.to_csv('combined_control_patient_data.csv', index=False)"
      ],
      "metadata": {
        "id": "GdcWELd1QSig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 :data preprocessing**"
      ],
      "metadata": {
        "id": "zZbZR6heVJto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données\n",
        "data = pd.read_csv('combined_control_patient_data.csv')\n",
        "\n",
        "# Fonction de nettoyage des données textuelles\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)  # Supprimer les caractères non alphanumériques\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Supprimer les espaces supplémentaires\n",
        "    return text\n",
        "\n",
        "# Nettoyage du texte\n",
        "data['text'] = data['text'].apply(clean_text)\n",
        "print(data.head(10))\n",
        "\n",
        "# Préparation de la tokenisation\n",
        "tokenizer = Tokenizer(num_words=121000)  # Limite à 12100 mots\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['text'])\n",
        "\n",
        "# Padding des séquences pour une longueur uniforme\n",
        "max_seq_length = 15  # Ajuster selon la longueur désirée\n",
        "X = pad_sequences(sequences, maxlen=max_seq_length)\n"
      ],
      "metadata": {
        "id": "OetSJvr2Zu25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9804a3a-2a6d-49c0-e2aa-60b2dc7c1109"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text conversation_id    group\n",
            "0        a family is preparing for a birthday party        wright16a  control\n",
            "1  the guests come over and walk through the door...       wright16a  control\n",
            "2                          the mother is very upset        wright16a  control\n",
            "3    and she takes a broom and tries to hit the dog        wright16a  control\n",
            "4  the little boy whose birthday it is is crying ...       wright16a  control\n",
            "5                 a woman has fallen into the water        wright16a  control\n",
            "6                       and there s a lot of rapids        wright16a  control\n",
            "7  a man wearing a life vest is trying to help th...       wright16a  control\n",
            "8  he has tied the branch to himself with the bel...       wright16a  control\n",
            "9  the woman is thankful that he is there to help...       wright16a  control\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to get num_words and max_seq_length**"
      ],
      "metadata": {
        "id": "avCxxitQV8ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer sans limiter le nombre de mots\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "\n",
        "# Nombre total de mots uniques\n",
        "word_count = len(tokenizer.word_index)\n",
        "print(f\"Nombre total de mots uniques dans le jeu de données : {word_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxhcvif-QbZe",
        "outputId": "0cc32d63-365b-4833-c4da-927568f70ecd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de mots uniques dans le jeu de données : 12092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_lengths = [len(text.split()) for text in data['text']]\n",
        "print(f\"Moyenne: {np.mean(text_lengths)}\")\n",
        "print(f\"Médiane: {np.median(text_lengths)}\")\n",
        "print(f\"Percentile 90: {np.percentile(text_lengths, 90)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jJF9XKfO7rQ",
        "outputId": "c601c453-94c2-4122-ab6d-1878d5a0143c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moyenne: 7.701700291841137\n",
            "Médiane: 6.0\n",
            "Percentile 90: 15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 : Word Embeddings preparation**"
      ],
      "metadata": {
        "id": "KzRYev-heu2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des embeddings GloVe\n",
        "EMBEDDING_DIM = 100  # Par exemple, pour GloVe 6B avec des vecteurs de 100 dimensions\n",
        "embedding_index = {}\n",
        "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Préparation de la matrice d'embedding\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Création de la couche d'embedding en utilisant la matrice d'embedding\n",
        "embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=max_seq_length,\n",
        "                            trainable=False)"
      ],
      "metadata": {
        "id": "tsnVYzk3WJZx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 : Data split**"
      ],
      "metadata": {
        "id": "_JwYGYFEeksi"
      }
    }
  ]
}