{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "pmshiijttrVw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klxSwkvqtxcU",
        "outputId": "26c0cdc2-5942-410d-893e-9eb46e81c871"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.losses import mean_squared_error"
      ],
      "metadata": {
        "id": "rQum3jsn0Bwo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWj7ydaZrGp1",
        "outputId": "7e3d5956-6edf-4eae-e280-44948a13441d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/703_Project/LSTM\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/703_Project/LSTM/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create combined csv files**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n8lPKfzW72qN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_folder(main_folder_path, group_label):\n",
        "    \"\"\" Traite tous les fichiers CSV dans un dossier et retourne un DataFrame fusionné. \"\"\"\n",
        "    frames = []  # Liste pour stocker les DataFrames de chaque fichier\n",
        "    for subdir, dirs, files in os.walk(main_folder_path):\n",
        "      for filename in files:\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(subdir, filename)\n",
        "            df = pd.read_csv(file_path)\n",
        "            df['conversation_id'] = filename.split('.')[0]  # Utilisez le nom du fichier comme ID\n",
        "            df['group'] = group_label\n",
        "            df = df[['text', 'conversation_id', 'group']]\n",
        "            frames.append(df)\n",
        "    return pd.concat(frames)\n",
        "\n",
        "# Chemins des dossiers pour les fichiers CSV des groupes contrôle et patient\n",
        "control_main_folder_path = \"Control_csv_data_clean\"\n",
        "patient_main_folder_path = \"Patient_csv_data_clean\"\n",
        "\n",
        "# Traitement des dossiers et fusion des DataFrames\n",
        "control_df = process_folder(control_main_folder_path, 'control')\n",
        "patient_df = process_folder(patient_main_folder_path, 'patient')\n",
        "combined_df = pd.concat([control_df, patient_df])\n",
        "\n",
        "# Enregistrement du DataFrame fusionné dans un nouveau fichier CSV\n",
        "combined_df.to_csv('combined_control_patient_data.csv', index=False)"
      ],
      "metadata": {
        "id": "GdcWELd1QSig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 : data split and preprocessing**"
      ],
      "metadata": {
        "id": "zZbZR6heVJto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données\n",
        "data = pd.read_csv('combined_control_patient_data.csv')\n",
        "data_df = pd.DataFrame(data)\n",
        "\n",
        "# Fonction de nettoyage des données textuelles\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9']\", \" \", text)  # Supprimer les caractères non alphanumériques\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Supprimer les espaces supplémentaires\n",
        "    return text\n",
        "\n",
        "# Nettoyage du texte\n",
        "data_df['text'] = data_df['text'].apply(clean_text)\n",
        "#print(data_df.head(10))\n",
        "\n",
        "# Fonction pour calculer le score de sentiment\n",
        "def sentiment_score(text):\n",
        "    analysis = TextBlob(text)\n",
        "    return analysis.sentiment.polarity  # Retourne un score entre -1 et 1\n",
        "\n",
        "# Appliquer la fonction à chaque réponse\n",
        "data_df['sentiment'] = data_df['text'].apply(sentiment_score)\n",
        "\n",
        "# Ici, 'sentiment' est la colonne des labels que nous venons de créer\n",
        "y = data_df['sentiment'].values\n",
        "\n",
        "# Division des données tout en conservant les identifiants de conversation\n",
        "X_train, X_temp, y_train, y_temp, ids_train, ids_temp = train_test_split(data_df.drop(columns=['sentiment']), data_df['sentiment'], data_df['conversation_id'], test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test, ids_val, ids_test = train_test_split(X_temp, y_temp, ids_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Préparation de la tokenisation\n",
        "tokenizer = Tokenizer(num_words=12400)  # Limite à 12400 mots\n",
        "tokenizer.fit_on_texts(data_df['text'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data_df['text'])\n",
        "\n",
        "\n",
        "# Tokenization et Padding des ensembles\n",
        "max_seq_length = 15  # Ajuster selon la longueur désirée\n",
        "X_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train['text']), maxlen=max_seq_length)\n",
        "X_val_padded = pad_sequences(tokenizer.texts_to_sequences(X_val['text']), maxlen=max_seq_length)\n",
        "X_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_test['text']), maxlen=max_seq_length)"
      ],
      "metadata": {
        "id": "OetSJvr2Zu25"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "data_sample_df['text'] = data_sample_df['text'].apply(clean_text)\n",
        "sample_sequences = tokenizer.texts_to_sequences(data_sample_df['text'])\n",
        "sample_padded = pad_sequences(sample_sequences, maxlen=max_seq_length)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8SWsv9tawIXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# examiner les séquences tokenisées et les longueurs de séquence\n",
        "for i in range(5):\n",
        "    print(f\"Texte original: {data['text'].iloc[i]}\")\n",
        "    print(f\"Séquence tokenisée: {sequences[i]}\")\n",
        "    print(f\"Longueur de la séquence: {len(sequences[i])}\\n\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "n89T2tUZzk6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to get num_words and max_seq_length**"
      ],
      "metadata": {
        "id": "avCxxitQV8ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer sans limiter le nombre de mots\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_df['text'])\n",
        "\n",
        "# Nombre total de mots uniques\n",
        "word_count = len(tokenizer.word_index)\n",
        "print(f\"Nombre total de mots uniques dans le jeu de données : {word_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxhcvif-QbZe",
        "outputId": "46283aa6-b720-425d-9426-1e95a8c55577"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de mots uniques dans le jeu de données : 12385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_lengths = [len(text.split()) for text in data_df['text']]\n",
        "print(f\"Moyenne: {np.mean(text_lengths)}\")\n",
        "print(f\"Médiane: {np.median(text_lengths)}\")\n",
        "print(f\"Percentile 90: {np.percentile(text_lengths, 90)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jJF9XKfO7rQ",
        "outputId": "9ba8d352-b954-420e-d77b-e43d749d50e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moyenne: 7.460360360360361\n",
            "Médiane: 6.0\n",
            "Percentile 90: 15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verifications**"
      ],
      "metadata": {
        "id": "MlDBj_WxzwZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérification des dimensions pour l'entraînement du modèle\n",
        "\n",
        "print(f\"Dimensions de X_train_padded: {X_train_padded.shape}\")\n",
        "print(f\"Dimensions de y_train: {y_train.shape}\")\n",
        "print(f\"Dimensions de X_test_padded: {X_test_padded.shape}\")\n",
        "print(f\"Dimensions de y_test: {y_test.shape}\")\n",
        "print(f\"Dimensions de X_val_padded: {X_val_padded.shape}\")\n",
        "print(f\"Dimensions de y_val: {y_val.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrBLloP80xwV",
        "outputId": "6b6c670e-c8a9-4469-fc81-7012f415f16c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions de X_train_padded: (63048, 15)\n",
            "Dimensions de y_train: (63048,)\n",
            "Dimensions de X_test_padded: (7881, 15)\n",
            "Dimensions de y_test: (7881,)\n",
            "Dimensions de X_val_padded: (7881, 15)\n",
            "Dimensions de y_val: (7881,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dimensions de ids_train: {ids_train.shape}\")\n",
        "print(f\"Dimensions de ids_val: {ids_val.shape}\")\n",
        "print(f\"Dimensions de ids_test: {ids_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyTwmg2mpYws",
        "outputId": "a3460d37-789e-406e-aade-1766b4e1d5b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions de ids_train: (63048,)\n",
            "Dimensions de ids_val: (7881,)\n",
            "Dimensions de ids_test: (7881,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 : Word Embeddings preparation**"
      ],
      "metadata": {
        "id": "KzRYev-heu2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des embeddings GloVe\n",
        "EMBEDDING_DIM = 100  # Par exemple, pour GloVe 6B avec des vecteurs de 100 dimensions\n",
        "embedding_index = {}\n",
        "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Préparation de la matrice d'embedding\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Création de la couche d'embedding en utilisant la matrice d'embedding\n",
        "embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=max_seq_length,\n",
        "                            trainable=False)"
      ],
      "metadata": {
        "id": "tsnVYzk3WJZx"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#contrôler la matrice d'embedding\n",
        "print(\"Dimensions de la matrice d'embedding:\", embedding_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObajGf7Q0mAS",
        "outputId": "792600e8-e052-4657-f6d0-fbd076a00187"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions de la matrice d'embedding: (12386, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verifications**"
      ],
      "metadata": {
        "id": "pwR3XCzJ2EkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Vocabulary\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    print(f\"Mot : {word}, Index : {index}\")\n",
        "    if index == 100:  # Afficher les 10 premiers mots\n",
        "        break\n"
      ],
      "metadata": {
        "id": "SaMPliXW1zQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correspondence in the Embedding Matrix\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if word in embedding_index:\n",
        "        print(f\"Le mot '{word}' est dans GloVe.\")\n",
        "    else:\n",
        "        print(f\"Le mot '{word}' n'est pas dans GloVe.\")\n",
        "    if index == 100:  # Vérifier pour les 10 premiers mots\n",
        "        break\n"
      ],
      "metadata": {
        "id": "C9L-nZ0N15Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coverage %\n",
        "total_mots = len(tokenizer.word_index)\n",
        "mots_dans_glove = sum(1 for word in tokenizer.word_index if word in embedding_index)\n",
        "pourcentage_couverture = (mots_dans_glove / total_mots) * 100\n",
        "print(f\"Pourcentage de mots du tokeniseur couverts par GloVe : {pourcentage_couverture}%\")\n"
      ],
      "metadata": {
        "id": "7w7HX2FK17_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 : LSTM model**"
      ],
      "metadata": {
        "id": "X_jpDytJrLRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle LSTM\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)  # Utilisez la couche d'embedding préparée précédemment\n",
        "model.add(LSTM(64, return_sequences=False))  # return_sequences=False pour la dernière couche LSTM\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='tanh'))  # Score de sentiment entre -1 et 1\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n"
      ],
      "metadata": {
        "id": "zrfjmHAF3vjd"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "grouped = data.groupby('conversation_id')  # Groupez par ID de conversation\n",
        "X = []\n",
        "\n",
        "for _, group in grouped:\n",
        "    # Tokenisez et paddez chaque réponse dans la conversation\n",
        "    sequences = tokenizer.texts_to_sequences(group['text'])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length)\n",
        "    X.append(padded_sequences)\n",
        "\n",
        "y = []\n",
        "\n",
        "for _, group in grouped:\n",
        "    sentiment_scores = group['sentiment'].values  # Scores de sentiment pour la conversation\n",
        "    y.append(sentiment_scores)\n",
        "\n",
        "print(y)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Gr3eh7Tk8G-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4 : Model training**"
      ],
      "metadata": {
        "id": "hFwXnEfjrSDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Entraînement du modèle avec Early Stopping\n",
        "model.fit(X_train_padded, y_train, epochs=3, batch_size=32, validation_data=(X_val_padded, y_val), callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuSI6ZVQEogo",
        "outputId": "c778801d-7d25-4b02-b42c-f41f1a7179bf"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1971/1971 [==============================] - 29s 13ms/step - loss: 0.0299 - val_loss: 0.0204\n",
            "Epoch 2/3\n",
            "1971/1971 [==============================] - 27s 14ms/step - loss: 0.0179 - val_loss: 0.0150\n",
            "Epoch 3/3\n",
            "1971/1971 [==============================] - 25s 12ms/step - loss: 0.0144 - val_loss: 0.0130\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78fb66707790>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5 : Results analysis**"
      ],
      "metadata": {
        "id": "vuDShrV4ra5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation du Modèle sur l'Ensemble de Test\n",
        "test_loss = model.evaluate(X_test_padded, y_test)\n",
        "print(f\"Perte sur l'ensemble de test: {test_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYTUroeuT0i_",
        "outputId": "6610c0f8-6ccd-4af0-bcf3-56d397a6a241"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "247/247 [==============================] - 2s 7ms/step - loss: 0.0984\n",
            "Perte sur l'ensemble de test: 0.09841075539588928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prédiction des Sentiments sur l'Ensemble de Test\n",
        "pred_sentiment = model.predict(X_test_padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sq4VckgaY6g",
        "outputId": "8cceef16-7379-417d-9b4e-591bf395bc15"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "247/247 [==============================] - 2s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer un DataFrame pour associer les prédictions aux conversations\n",
        "test_df = pd.DataFrame({\n",
        "    'text' : X_test['text'],\n",
        "    'conversation_id': ids_test,\n",
        "    'group' : X_test['group'],\n",
        "    'predicted_sentiment': pred_sentiment.flatten()\n",
        "})\n",
        "\n",
        "# Grouper par conversation_id\n",
        "conversations = test_df.groupby('conversation_id')\n",
        "evolution_sentiments = {}\n",
        "\n",
        "for conversation_id, group in conversations:\n",
        "    sentiment_debut = group['predicted_sentiment'].iloc[0]\n",
        "    sentiment_fin = group['predicted_sentiment'].iloc[-1]\n",
        "    evolution_sentiments[conversation_id] = sentiment_fin - sentiment_debut"
      ],
      "metadata": {
        "id": "xJvNYj0celpY"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDbQcQoW5gTR",
        "outputId": "a0dac79d-d249-4b8b-daa3-657e429a5bf9"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'conversation_id', 'group', 'predicted_sentiment'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('test_df.csv', index=\"False\")"
      ],
      "metadata": {
        "id": "A3feeWFoAGOg"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evolutions_patient = [evolution_sentiments[cid] for cid in conversations.indices if data_df[data_df['conversation_id'] == cid]['group'].iloc[0] == 'patient']\n",
        "evolutions_control = [evolution_sentiments[cid] for cid in conversations.indices if data_df[data_df['conversation_id'] == cid]['group'].iloc[0] == 'control']\n"
      ],
      "metadata": {
        "id": "Gg5w3KZNfvnu"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.to_csv('data_df_final.csv', index = 'False')"
      ],
      "metadata": {
        "id": "MLOq205vdoFJ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "156kEnIpbvlf"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test sample**"
      ],
      "metadata": {
        "id": "YxrSwRuKdDE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sample = pd.read_csv('test.csv', sep=';')\n",
        "data_sample_df = pd.DataFrame(data_sample)\n",
        "#print(data_sample.head())"
      ],
      "metadata": {
        "id": "QahPFiyzs5bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger l'échantillon de données\n",
        "# Appliquez les mêmes étapes de nettoyage et de tokenisation\n",
        "data_sample_df['text'] = data_sample_df['text'].apply(clean_text)\n",
        "sample_sequences = tokenizer.texts_to_sequences(data_sample_df['text'])\n",
        "sample_padded = pad_sequences(sample_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# Prédire les sentiments\n",
        "predicted_sentiments = model.predict(sample_padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1gL0biddHvE",
        "outputId": "6c0ece06-7b19-4d0a-853f-17f1ec521c66"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_sample['predicted_sentiment'] = predicted_sentiments.flatten()"
      ],
      "metadata": {
        "id": "tlg73Cr2dQB4"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversations = data_sample.groupby('conversation_id')\n",
        "evolution_sentiments = {}\n",
        "\n",
        "for conversation_id, group in conversations:\n",
        "    sentiment_debut = group['predicted_sentiment'].iloc[0]\n",
        "    sentiment_fin = group['predicted_sentiment'].iloc[-1]\n",
        "    evolution_sentiments[conversation_id] = sentiment_fin - sentiment_debut"
      ],
      "metadata": {
        "id": "itganbOAdYTu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evolutions_aphasique = [evolution_sentiments[cid] for cid in conversations.indices if data_sample[data_sample['conversation_id'] == cid]['group'].iloc[0] == 'patient']\n",
        "evolutions_non_aphasique = [evolution_sentiments[cid] for cid in conversations.indices if data_sample[data_sample['conversation_id'] == cid]['group'].iloc[0] == 'control']\n"
      ],
      "metadata": {
        "id": "ykNl67DPdbxh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_sample.to_csv('data_sample.csv', index = 'False')"
      ],
      "metadata": {
        "id": "2X4b63bAdj_G"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}